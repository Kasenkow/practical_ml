---
title: "Weight Lifting Quality Assessment"
author: "Anton Kasenkov"
date: "03 02 2020"
output:
        html_document:
                code_folding: hide
                keep_md: true
bibliography: velloso2013.bib
link-citations: yes
---
<style>
body {
text-align: justify}
</style>

```{r setup, message = FALSE, results = "hide"}
knitr::opts_chunk$set(echo = TRUE,
                      cache = TRUE,
                      fig.width = 12,
                      fig.height = 8,
                      warning = FALSE,
                      message = FALSE)

pacman::p_load(tidyverse, caret, doParallel)
select <- dplyr::select
```

## Introduction
The ability to automatically classify physical activity movements into categories in respect of their proper performance form (so-called qualitative activity recognition) can have potential benefits in reduction of weightlifting-related injuries.

This project is aimed at identifying correctly the type of the form in which a simple weightlifting excercise (unilateral bicep curl with dumbbell) was performed. There are 5 types of form, 1 of which was correct, the others illustrated common mistakes.

The results show a great quality of classification.

## Data preparation
First of all, I have to get the data. This particular dataset was generously provided by [@velloso2013qualitative].
```{r getdata}
url_train = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
url_test = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(url_train, "data/train_test.csv")
download.file(url_test, "data/test.csv")
```

Now I can prepare the data. I would use a simple function that removes from dataset variables with > 90% missing values. Also, I would transform our target feature "classe" into a factor variable and remove 7 id variables from both training and testing datasets [^1], since they should not be related to the data classification (i.e. performance should not depend on time of the performance and/or performer).
```{r prepdata}
# function to remove almost empty variables (containing > threshold NAs):
remalmempty <- function(df, threshold = 0.9) {
        naprop = sapply(df, function(x) round(sum(is.na(x)) / length(x), 2))
        return(df[naprop < threshold])
}

train_df <-
        read_csv("data/train_test.csv",
                 na = c("#DIV/0!", "NA", "", " ")) %>% # additional NA values
        remalmempty %>% # removing vars with > 90% NAs
        mutate_at("classe", ~factor(.)) %>% # recoding target feature into factor
        select(-(1:7)) # removing 7 irrelevant to performance fetures

test_df <-
        read_csv("data/test.csv") %>%
        remalmempty %>% # removing vars with > 90% NAs
        select(-(1:7)) # removing 7 irrelevant to performance fetures
```

I would partition our training dataframe into 2 smaller sets: one for model training and the other - for model performance assessment. The proportion is 3/4.
```{r testtrain}
# test and train split:
set.seed(2020)
inTrain <- createDataPartition(train_df$classe, p = 0.75, list = FALSE)
training <- train_df[inTrain, ]
testing <- train_df[-inTrain, ]
```

## Data exploration
For exploratory purposes I will check if there are any missing values left, I would also look at the scale of our data to decide if addtional preprocessing is needed during the modelling phase. I should also check if our classes are balanced.
```{r explore}
anyNA(training) # NA check
sapply(training[-ncol(training)], sd) # need scaling
table(training$classe) # classes are balanced
```

I've also made several plots, that show somewhat complicated pattern. This is just one of them:
```{r plot}
training %>%
mutate_if(is.numeric, scale) %>% 
        ggplot(aes(x = roll_belt, y = yaw_belt, color = classe))+
        geom_point(alpha = 0.5)+
        theme_bw()
```

## Modelling
For modelling I chose random forest algorithm because the data does not appear to contain linear patterns and because random forest is good at predicting tasks. To speed up the process I used doParallel package.
```{r parallel}
cl <- makePSOCKcluster(5)
registerDoParallel(cl)
```

To reduce the out of sample error I chose 10-times repeated 5-fold cross-validation. To account for different scales I also centered and scaled the data.
```{r training}
# Creating object for Accuracy training:
set.seed(2020)
objControl <-
        trainControl(method = "repeatedcv",
                     number = 10,
                     repeats = 5)
rf_mdl <-
        train(classe ~ .,
              data = training,
              trainControl = objControl,
              method = "rf",
              preProcess = c("center", "scale"))
```

## Prediction

Initially, I intended to explore ensembling models, but after I checked with the confusion table, I decided not to go further with algorithm complication.
```{r confmat}
confusionMatrix(predict(rf_mdl, newdata = testing), testing$classe)$table
# I am happy with this result
stopCluster(cl)
```

Thus, my estimate of the out of sample error with 95% CI is anywhere between 99,0% to 99,5%:
```{r oose}
# Expected out of sample error:
round(confusionMatrix(predict(rf_mdl, newdata = testing), testing$classe)$overal[c(1, 3, 4)], 3)
```

[^1]: 7 id variables are the username of the performer, 3 types of timestamp and 2 variables of window.

## References
